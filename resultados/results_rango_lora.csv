Empresa,Dataset,Modelo,Base/Fine-tuning,Capas LoRa,Tama침o batch,Learning rate,Iters,Adapter,Lora_Rank,Accuracy Entrenamiento,Accuracy Validaci칩n,Accuracy Test,EC Entrenamiento,EC Validaci칩n,EC Testeo,ECN Entrenamiento,ECN Validaci칩n,ECN Testeo
Microsoft,hellaswag,phi 2,Fine-tuning,16,5,1.0e-05,5,Best,16,0.7335,0.7245508982035929,0.6668372569089048,0.6740723252296448,0.71387779712677,0.8734283447265625,0.06216376547836428,0.0658346684723181,0.0805486259066993
Microsoft,hellaswag,phi 2,Fine-tuning,16,5,1.0e-05,5,Best,1,0.391,0.4166666666666667,0.413,1.4782980680465698,1.4340946674346924,1.4679317474365234,0.13633043661577177,0.1322539338924368,0.13537444198558587
Microsoft,hellaswag,phi 2,Fine-tuning,16,5,1.0e-05,5,Best,8,0.769,0.77,0.6820512820512821,0.5466697812080383,0.5837387442588806,0.7434129118919373,0.05041455314208822,0.053833088600605775,0.06855843828434018
Microsoft,hellaswag,phi 2,Fine-tuning,16,5,1.0e-05,5,Best,64,0.7335,0.7245508982035929,0.6668372569089048,0.6740723252296448,0.71387779712677,0.8734283447265625,0.06216376547836428,0.0658346684723181,0.0805486259066993
Microsoft,hellaswag,phi 2,Fine-tuning,16,5,1.0e-05,5,Best,4,0.391,0.4166666666666667,0.413,1.4782980680465698,1.4340946674346924,1.4679317474365234,0.13633043661577177,0.1322539338924368,0.13537444198558587
Microsoft,hellaswag,phi 2,Fine-tuning,16,5,1.0e-05,5,Best,2,0.391,0.4166666666666667,0.413,1.4782980680465698,1.4340946674346924,1.4679317474365234,0.13633043661577177,0.1322539338924368,0.13537444198558587
