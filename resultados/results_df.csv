,Empresa,Dataset,Modelo,Base/Fine-tuning,Capas LoRa,Tamaño batch,Learning rate,Iters,Adapter,Accuracy,Proportion Targets in Softmax,Accuracy restringido,Entropía cruzada,Entropía cruzada normalizada
0,mistralai,socialiqa,Mistral-7B-v0.1,Base,n/a,n/a,n/a,5000,n/a,0.42374616171954965,0.40131015,0.574718526100307,10.373392105102539,0.9999905308192873
1,mistralai,hellaswag,Mistral-7B-v0.1,Fine-tuning,2,1,1e-5,5000,Best,0.34,0.9851823,0.34,10.37020492553711,0.9996833799866732
2,microsoft,socialiqa,phi-1.5,Fine-tuning,16,4,1e-5,5000,Best,0.6553846153846153,0.9988314,0.6553846153846153,10.843157768249512,0.9999691299011999
3,microsoft,socialiqa,phi-1.5,Fine-tuning,8,4,1e-5,5000,Best,0.6112820512820513,0.9973381,0.6112820512820513,10.843201637268066,0.9999731755551736
4,microsoft,socialiqa,phi-1.5,Fine-tuning,24,4,1e-5,5000,Best,0.6266666666666667,0.998487,0.6266666666666667,10.843160629272461,0.9999693937481982
5,microsoft,socialiqa,phi-1.5,Base,n/a,n/a,n/a,5000,n/a,0.5905834186284544,0.990771,0.5905834186284544,10.84314250946045,0.9999678106633774
6,microsoft,hellaswag,phi-1.5,Fine-tuning,16,5,1e-5,2000,Best,0.348,0.99256325,0.348,10.843218803405762,0.9999747586371635
7,microsoft,hellaswag,phi-1.5,Fine-tuning,8,5,1e-4,5000,Best,0.397,0.99852884,0.397,10.843174934387207,0.9999707129831896
8,microsoft,hellaswag,phi-1.5,Fine-tuning,8,5,1e-5,1000,Best,0.383,0.97873485,0.383,10.843155860900879,0.999968954003201
9,microsoft,hellaswag,phi-1.5,Fine-tuning,8,5,1e-5,1000,Best,0.383,0.97873485,0.383,10.843155860900879,0.999968954003201
10,microsoft,hellaswag,phi-1.5,Fine-tuning,2,5,1e-5,1000,Best,0.367,0.9033124,0.367,10.8432035446167,0.9999733514531726
