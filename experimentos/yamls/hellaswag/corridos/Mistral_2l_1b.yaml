model: "mistralai/Mistral-7B-v0.1"
data: "hellaswag"
iters: 2000
adapter-path: "adapters"
seed: 0
lora-layers: 1
batch-size: 4
based-model: False
learning-rate: 1e-5